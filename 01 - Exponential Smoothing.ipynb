{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a088e09c-17c3-44d7-8d30-ece17874f15e",
   "metadata": {},
   "source": [
    "# Exponential Smoothing\n",
    "\n",
    "Most of the material in this notebook is adapted from [Forecasting: Principles and Practice](https://otexts.com/fpp2/expsmooth.html)\n",
    "\n",
    "How do you handle forecasts when you just don't have that much information? Why do so many forecasting methods lag behind the time series data that they seek to predict? If you want the solution to these problems, as well as a model that is often chosen by practitioners over our other time series analysis tools, then you want **exponential smoothing** models!\n",
    "\n",
    "## Simple and Smooth\n",
    "\n",
    "Let's start off with a simple time series: $ y_1, y_2, ..., y_{t-1}, y_t $. If we want to predict $y_{t+1}$, then we need to choose some method of using the information that we have about previous periods to make our best guess at the following period. In this case, we are not going to use any parametric assumptions, we are simply going to choose weights for each observation, and then create a predicted outcome based on those weights.\n",
    "\n",
    "### The Naive Model\n",
    "\n",
    "The simplest weighting scheme is to assign all importance to the first observation, and no importance to any other observation in the time series. This model, called the **naive model**, will simply use the most recent observation of a time series as the prediction for the next event. You can write it like this:\n",
    "\n",
    "$$ y_{t+1} = y_t + \\epsilon_{t+1} $$\n",
    "\n",
    "Probably not a great model, since there is almost certainly information to be gained by utilizing more than a single observation to predict our next step. This model does have the advantage of being available at the start of data collection, however. We only need a single past observation to begin making **some** forecast!\n",
    "\n",
    "### Just make an average\n",
    "\n",
    "Another weighting scheme we might choose is a simple average. Every past observation is equally weighted, and the average value of past periods is our predicted value for the subsequent period.\n",
    "\n",
    "$$ y_{t+1} = \\frac{1}{t}\\sum_{i=1}^t y_i$$\n",
    "\n",
    "In some cases, this might be a useful predictor, but for many time series, this simply isn't enough. We need to adjust the **importance** of our observations to match their relevance for the next time period.\n",
    "\n",
    "### Exponential Decay\n",
    "\n",
    "One simple way to create a model wherein each observation has less importance as we move further into the past is to use an **exponential decay** function to weight our observations. Like our average model from above, the overall weight of all observations will still sum to 1 (meaning that this is just a weighted average model), with the most recent observations being vastly (exponentially??) more important to our prediction than the earliest observations. The equation representing this kind of weighted average looks like this:\n",
    "\n",
    "$$ y_{t+1} = \\alpha \\cdot y_t + \\alpha \\cdot(1-\\alpha) \\cdot y_{t-1} + \\alpha \\cdot(1-\\alpha)^2 \\cdot y_{t-2} + ... + \\alpha \\cdot(1-\\alpha)^n \\cdot y_{t-n} $$\n",
    "\n",
    "\n",
    "A simple table (taken from [here](https://otexts.com/fpp2/ses.html)) reflecting the weights that would be assigned to various observations using this kind of weighting scheme helps to clarify a bit.\n",
    "\n",
    "|    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;       | $\\alpha=0.2$  | $\\alpha=0.4$  | $\\alpha=0.6$  | $\\alpha=0.8$  |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| $y_t$       | 0.2000 | 0.4000 | 0.6000 | 0.8000 |\n",
    "| $y_{t−1}$      | 0.1600 | 0.2400 | 0.2400 | 0.1600 |\n",
    "| $y_{t−2}$      | 0.1280 | 0.1440 | 0.0960 | 0.0320 |\n",
    "| $y_{t−3}$      | 0.1024 | 0.0864 | 0.0384 | 0.0064 |\n",
    "| $y_{t−4}$      | 0.0819 | 0.0518 | 0.0154 | 0.0013 |\n",
    "| $y_{t−5}$ | &nbsp;&nbsp;&nbsp;&nbsp;0.0655 | &nbsp;&nbsp;&nbsp;&nbsp;0.0311 | &nbsp;&nbsp;&nbsp;&nbsp;0.0061 | &nbsp;&nbsp;&nbsp;&nbsp;0.0003 |\n",
    "\n",
    "What is $\\alpha$? It's called our **smoothing parameter**, and it dictates the speed at which our weights fall over time. For very high $\\alpha$ values, the weight is primarily placed on more recent observations, and for low $\\alpha$ values, the weight is more evenly spread, though more recent values still receive greater \"attention\" than observations from the more distant past. $\\alpha$ values need to be between 0 and 1, with 0 denoting the average model, and 1 the naive model.\n",
    "\n",
    "Another way to write the model is to break it into its components. In this case, we have a simple model with only two components: the level ($l_t$) and forecast components ($\\hat{y}_{t+1}$).\n",
    "\n",
    "| Component | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Term |\n",
    "| --- | --- |\n",
    "| Forecast | $ \\hat{y}_{t+1} = l_t $ |\n",
    "| Level | $ l_t = \\alpha \\cdot y_t + \n",
    "(1-\\alpha) \\cdot l_{t-1}$|\n",
    "\n",
    "Essentially, in our basic exponential smoothing model, we calculate the average level of the time series, and use this to make our prediction of the next period (or periods). Because we are already using all information that we believe is relevant to the problem, the level does not change as we move further into the future. This means that predictions of **any** future period will be the same as the prediction for $t+1$, until we accumulate further observations.\n",
    "\n",
    "### Improving the forecast with trends\n",
    "\n",
    "We can improve on our preliminary exponential smoothing model by incorporating trend information. Trend information is a simple estimate of the most recent direction and magnitude of movement, and can be readily incorporated into our smoothing model through a third component included in our model. Our first option is to simply include a linear trend in the model. The linear trend has the advantage of simple implementation and ease of interpretation.\n",
    "\n",
    "| Component | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Term |\n",
    "| --- | --- |\n",
    "| Forecast | $ \\hat{y}_{t+h} = l_t + h\\cdot b_t$ |\n",
    "| Level | $ l_t = \\alpha \\cdot y_t + (1-\\alpha) \\cdot (l_{t-1} + b_{t-1})$|\n",
    "| Trend | $ b_t = \\beta \\cdot (l_t - l_{t-1}) + (1-\\beta) \\cdot b_{t-1} $|\n",
    "\n",
    "Thus our model with linear trend is the same as our model from before, with the inclusion of a trend component. The trend component is the weighted average of the difference between the current and previous level, and the slope term from the previous period. We smooth the trend, just like we smooth the level of our model. This allows us to adjust for the most recent data, but also to include information from previous periods, since those also contain information about possible future values of our time series.\n",
    "\n",
    "A linear trend is unlikely to persist far into the future. Many exponential smoothing models accomodate the unlikely progression of linear trends through **damping** techniques. Essentially, the model has a built-in decay function to dampen the effect of the trend as we look further into the future. The equations would be adapted as follows:\n",
    "\n",
    "| Component | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Term |\n",
    "| --- | --- |\n",
    "| Forecast | $ \\hat{y}_{t+h} = l_t + (\\phi + \\phi^2 + ... + \\phi^h) \\cdot b_t$ |\n",
    "| Level | $ l_t = \\alpha \\cdot y_t + (1-\\alpha) \\cdot (l_{t-1} + \\phi \\cdot b_{t-1})$|\n",
    "| Trend | $ b_t = \\beta \\cdot (l_t - l_{t-1}) + (1-\\beta) \\cdot  \\phi \\cdot b_{t-1} $|\n",
    "\n",
    "These updated trend equations incorporate the decay term ($\\phi$), which denotes the speed with which the linear trend decays to 0 in future observations.\n",
    "\n",
    "### Seasonality and exponential smoothing\n",
    "\n",
    "Our last addition to the exponential smoothing model (at least in this course) is to incorporate seasonality. In order to do so, we will incorporate one more term into our model. At the same time we also have to provide information on the number of observations observed per seasonal cycle. If our data is quarterly, and seasons happen over the course of a single year, then we would say that our seasonal term ($m$) is 4 (or four observations). If we have data that displays daily seasonality with hourly observations, then $m=24$.\n",
    "\n",
    "| Component | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Term |\n",
    "| --- | --- |\n",
    "| Forecast | $ \\hat{y}_{t+h} = l_t + (\\phi + \\phi^2 + ... + \\phi^h) \\cdot b_t + s_{t+h-m(k+1)}$ |\n",
    "| Level | $ l_t = \\alpha \\cdot (y_t - s_{t-m}) + (1-\\alpha) \\cdot (l_{t-1} + \\phi \\cdot b_{t-1})$|\n",
    "| Trend | $ b_t = \\beta \\cdot (l_t - l_{t-1}) + (1-\\beta) \\cdot  \\phi \\cdot b_{t-1} $|\n",
    "| Seasonality | $s_t = \\gamma \\cdot (y_t - l_{t-1} - \\phi \\cdot b_{t-1}) + (1-\\gamma) \\cdot s_{t-m} $ | \n",
    "\n",
    "In essence, we incorporate a smoothed term that accounts for behavior at the same point in the **previous cycle**. Thus, we can account for the position in the current cycle, as well as the behavior within our current cycle, and any recent fluctuations that we have observed from one period to the next.\n",
    "\n",
    "This is a **lot** of math speak. Let's implement these models, and see if we can gain more intuition about their function.\n",
    "\n",
    "## Implementing Exponential Smoothing\n",
    "\n",
    "Let's start by importing some US economic data. We will focus on forecasting non-farm payroll figures over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd626a-d42e-40dc-b854-fe2a8b960826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/RecessionForecasting.csv\")\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "\n",
    "px.line(data, x=\"DATE\", y='CivEmpLevel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fcd1f-d3b1-470c-b13c-b8c446bab396",
   "metadata": {},
   "source": [
    "Once we have our data imported, we can start to implement the simple exponential smoothing model we described first, which will just be a weighted average, with a level term doing all the work of creating our forecasts. Thus, our forecasts should be totally flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24af9a-f761-4004-9a78-61e1e6c57c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment = data['CivEmpLevel']\n",
    "employment.index = data['DATE']\n",
    "employment.index.freq = employment.index.inferred_freq\n",
    "\n",
    "alpha020 = SimpleExpSmoothing(employment).fit(\n",
    "                                        smoothing_level=0.2,\n",
    "                                        optimized=False)\n",
    "\n",
    "alpha050 = SimpleExpSmoothing(employment).fit(\n",
    "                                        smoothing_level=0.5,\n",
    "                                        optimized=False)\n",
    "\n",
    "alpha080 = SimpleExpSmoothing(employment).fit(\n",
    "                                        smoothing_level=0.8,\n",
    "                                        optimized=False)\n",
    "\n",
    "forecast020 = alpha020.forecast(3)\n",
    "forecast050 = alpha050.forecast(3)\n",
    "forecast080 = alpha080.forecast(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637209a3-7425-43d8-bbb6-62bb0cd999fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plotting our data\n",
    "\n",
    "smoothData = pd.DataFrame([employment.values, alpha020.fittedvalues.values,  alpha050.fittedvalues.values,  alpha080.fittedvalues.values]).T\n",
    "smoothData.columns = ['Truth', 'alpha=0.2', 'alpha=0.5', 'alpha=0.8']\n",
    "smoothData.index = employment.index\n",
    "\n",
    "fig = px.line(smoothData, y = ['Truth', 'alpha=0.2', 'alpha=0.5', 'alpha=0.8'], \n",
    "        x = smoothData.index,\n",
    "        color_discrete_map={\"Truth\": 'blue',\n",
    "                           'alpha=0.2': 'red', \n",
    "                            'alpha=0.5':'green', \n",
    "                            'alpha=0.8':'purple'}\n",
    "       )\n",
    "\n",
    "fig.update_xaxes(range=[smoothData.index[-50], forecast020.index[-1]])\n",
    "fig.update_yaxes(range=[142000, 153000])\n",
    "\n",
    "\n",
    "# Incorporating the Forecasts\n",
    "\n",
    "fig.add_trace(go.Scatter(x=forecast020.index, y = forecast020.values, name='Forecast alpha=0.2', line={'color':'red'}))\n",
    "fig.add_trace(go.Scatter(x=forecast050.index, y = forecast050.values, name='Forecast alpha=0.5', line={'color':'green'}))\n",
    "fig.add_trace(go.Scatter(x=forecast080.index, y = forecast080.values, name='Forecast alpha=0.8', line={'color':'purple'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6726da-2563-477e-9622-21a0cba4b2cc",
   "metadata": {},
   "source": [
    "Above, we fitted the model with three different levels of smoothing, so that we can see the difference as we choose varied levels of $\\alpha$. If we want to simply choose the **best** smoothing parameter given the data that we can observe, we can simply choose to optimzie our model instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eaa9fe-3ca8-4257-8b7e-c8f4c257d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlined Modeling\n",
    "\n",
    "alphaBest = SimpleExpSmoothing(employment).fit()\n",
    "forecast = alphaBest.forecast(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad13e98-39e7-4375-bf26-1de6ce1f3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plotting our data\n",
    "\n",
    "smoothData = pd.DataFrame([employment.values, alphaBest.fittedvalues.values]).T\n",
    "smoothData.columns = ['Truth', 'Best Fit Model']\n",
    "smoothData.index = employment.index\n",
    "\n",
    "fig = px.line(smoothData, y = ['Truth', 'Best Fit Model'], \n",
    "        x = smoothData.index,\n",
    "        color_discrete_map={\"Truth\": 'blue',\n",
    "                           'Best Fit Model': 'red'}\n",
    "       )\n",
    "\n",
    "fig.update_xaxes(range=[smoothData.index[-50], forecast.index[-1]])\n",
    "fig.update_yaxes(range=[142000, 153000])\n",
    "\n",
    "\n",
    "# Incorporating the Forecasts\n",
    "\n",
    "fig.add_trace(go.Scatter(x=forecast.index, y = forecast.values, name='Forecast', line={'color':'red'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d2bb2-d494-479a-b28a-2c99020f4de7",
   "metadata": {},
   "source": [
    "Most of the time, though, we don't want just flat forecasts. Let's create a model with a trend component, now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9e616-6054-4c34-881f-e1b8d7a7d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear trend\n",
    "trend = ExponentialSmoothing(employment, trend='add').fit()\n",
    "# Linear trend with damping\n",
    "dampedTrend = ExponentialSmoothing(employment, trend='add', damped=True).fit(use_boxcox=True,use_brute=True)\n",
    "\n",
    "forecast_t = trend.forecast(10)\n",
    "forecast_dt = dampedTrend.forecast(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975bb7b9-b2de-4ac8-bb27-1012a6cf0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plotting our data\n",
    "\n",
    "smoothData = pd.DataFrame([employment.values, trend.fittedvalues.values, dampedTrend.fittedvalues.values]).T\n",
    "smoothData.columns = ['Truth', 'Trend', 'Damped Trend']\n",
    "smoothData.index = employment.index\n",
    "\n",
    "fig = px.line(smoothData, y = ['Truth', 'Trend', 'Damped Trend'], \n",
    "        x = smoothData.index,\n",
    "        color_discrete_map={\"Truth\": 'blue',\n",
    "                           'Trend': 'red',\n",
    "                            'Damped Trend': 'green'\n",
    "                           },\n",
    "              title='Linear and Damped Trends'\n",
    "       )\n",
    "\n",
    "fig.update_xaxes(range=[smoothData.index[-50], forecast_t.index[-1]])\n",
    "fig.update_yaxes(range=[142000, 154000])\n",
    "\n",
    "\n",
    "# Incorporating the Forecasts\n",
    "\n",
    "fig.add_trace(go.Scatter(x=forecast_t.index, y = forecast_t.values, name='Forecast Trend', line={'color':'red'}))\n",
    "fig.add_trace(go.Scatter(x=forecast_dt.index, y = forecast_dt.values, name='Forecast Damped Trend', line={'color':'green'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d6c19-4216-4c9b-b5be-d2131ef468b3",
   "metadata": {},
   "source": [
    "Our model might also benefit from seasonal adjustments, which we can also easily incorporate into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ac897-e1c3-4f56-9a5f-7336c8453af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear trend\n",
    "trend = ExponentialSmoothing(employment, trend='add', seasonal='add').fit()\n",
    "# Linear trend with damping\n",
    "dampedTrend = ExponentialSmoothing(employment, trend='mul', seasonal='add', damped=True).fit(use_boxcox=True,use_brute=True)\n",
    "\n",
    "forecast_t = trend.forecast(10)\n",
    "forecast_dt = dampedTrend.forecast(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d9b96-88f2-4088-a0e1-b71f48327b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plotting our data\n",
    "\n",
    "smoothData = pd.DataFrame([employment.values, trend.fittedvalues.values, dampedTrend.fittedvalues.values]).T\n",
    "smoothData.columns = ['Truth', 'Trend', 'Damped Trend']\n",
    "smoothData.index = employment.index\n",
    "\n",
    "fig = px.line(smoothData, y = ['Truth', 'Trend', 'Damped Trend'], \n",
    "        x = smoothData.index,\n",
    "        color_discrete_map={\"Truth\": 'blue',\n",
    "                           'Trend': 'red',\n",
    "                            'Damped Trend': 'green'\n",
    "                           },\n",
    "              title='With Seasonality'\n",
    "       )\n",
    "\n",
    "fig.update_xaxes(range=[smoothData.index[-50], forecast_t.index[-1]])\n",
    "fig.update_yaxes(range=[142000, 157000])\n",
    "\n",
    "\n",
    "# Incorporating the Forecasts\n",
    "\n",
    "fig.add_trace(go.Scatter(x=forecast_t.index, y = forecast_t.values, name='Forecast Trend', line={'color':'red'}))\n",
    "fig.add_trace(go.Scatter(x=forecast_dt.index, y = forecast_dt.values, name='Forecast Damped Trend', line={'color':'green'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba338a31-eb9f-447f-8f40-788074cab868",
   "metadata": {},
   "source": [
    "You'll notice (or I'll just point it out to help you notice) that most of the code above is formatting for the plots. It is actually very simple to create our Exponential Smoothing models, and even easier to forecast with them. \n",
    "\n",
    "As one of the most powerful tools in a practitioner's toolkit, it is great to have such a straightforward model with such simple implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5ce04-651f-4a7a-8155-f270ea49148d",
   "metadata": {},
   "source": [
    "**Reading Reflection**\n",
    "\n",
    "Describe a situation in which you might only have a few observations of time series data for an important metric in your current or anticipated career. After describing the data and context, describe which implementation of exponential smoothing you would expect to be most beneficial when forecasting that specific time series. Submit in Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
